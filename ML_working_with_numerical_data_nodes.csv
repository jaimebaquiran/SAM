:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
ML_working_with_numerical_data_v1,Machine Learning - Working with numerical data,Curriculum,,1.0.0,Transform your understanding of numerical data through dialogue,,2025-12-31,2025-12-31,18,,
MLWWND_001,Feature Vectors and Their Role in ML Models,Concept,Recognize that models do not act directly on raw dataset rows but instead ingest feature vectors;Define a feature vector as an array of numeric values representing one example;Explain that feature vectors typically contain processed values rather than raw dataset values;Understand that features are represented numerically (commonly as float tensors) and non-float inputs are typically cast or encoded to floats;Identify that feature engineering is the process of determining how to represent raw dataset values as trainable values in feature vectors,1.0.0,Understanding feature vectors and their role in ml models: Recognize that models do not act directly on raw dataset rows but instead ingest feature vectors,,2025-12-31,2025-12-31,,Foundational,
MLWWND_002,Numerical vs Categorical Data,Concept,"Define numerical data as integers or floating-point values that are additive, countable, and ordered;Identify examples of numerical data such as temperature, weight, and population counts;Recognize that some numbers like postal codes represent categories rather than mathematical relationships;Distinguish between data that behaves like numbers versus data that represents categories despite being numeric;Understand that numerical features can be directly used in mathematical operations while categorical features cannot",1.0.0,"Understanding numerical vs categorical data: Define numerical data as integers or floating-point values that are additive, countable, and ordered",,2025-12-31,2025-12-31,,Foundational,
MLWWND_003,Feature Engineering for Numerical Data,Concept,"Define feature engineering as the process of transforming raw dataset values into representations that models can better learn from;Recognize that models often produce better predictions from engineered features than from raw dataset values;Identify normalization as a technique for converting numerical values into a standard range;Identify binning (bucketing) as a technique for converting numerical values into discrete buckets of ranges;Understand that feature engineering can improve convergence speed, training stability, or accuracy on validation data;Recognize that feature engineering is a vital part of machine learning workflows",1.0.0,Understanding feature engineering for numerical data: Define feature engineering as the process of transforming raw dataset values into representations that models can better learn from,,2025-12-31,2025-12-31,,Foundational,
MLWWND_004,Data Exploration Through Visualization,Concept,"Recognize the importance of visualizing data using plots and graphs before creating feature vectors;Understand that graphs help identify anomalies, outliers, or patterns hiding in the data;Identify scatter plots as useful for examining relationships between two numerical features;Identify histograms as useful for examining the distribution of a single numerical feature;Recognize that visualizations should be used throughout data transformations, not just at the beginning;Choose a visualization appropriate to feature type and dataset size",1.0.0,Understanding data exploration through visualization: Recognize the importance of visualizing data using plots and graphs before creating feature vectors,,2025-12-31,2025-12-31,,Foundational,
MLWWND_005,Statistical Evaluation of Numerical Data,Concept,"Identify mean and median as basic statistics for evaluating potential features;Recognize standard deviation as an important statistical measure for understanding data spread;Understand that quartile divisions (0th, 25th, 50th, 75th, 100th percentiles) help characterize data distribution;Recognize that the 0th percentile represents the minimum value and the 100th percentile represents the maximum value;Understand that the 50th percentile is equivalent to the median;Recognize that basic statistics alone may not reveal all anomalies and should be combined with visualization",1.0.0,Understanding statistical evaluation of numerical data: Identify mean and median as basic statistics for evaluating potential features,,2025-12-31,2025-12-31,,Foundational,
MLWWND_006,Outlier Detection and Management,Concept,"Define an outlier as a value distant from most other values in a feature or label;Identify outliers using the IQR method: values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR;Identify outliers using z-score thresholds: values with |z-score| > 3 are typically considered outliers;Confirm suspected outliers through visual inspection using box plots or histograms;Recognize that outliers due to mistakes (data entry errors, instrument malfunction) should generally be deleted;Understand that legitimate outliers should be kept if the model needs to make predictions on similar data points;Identify that extreme outliers can be handled through deletion, clipping, or robust scaling techniques",1.0.0,Understanding outlier detection and management: Define an outlier as a value distant from most other values in a feature or label,,2025-12-31,2025-12-31,,Intermediate,
MLWWND_007,Goals and Benefits of Normalization,Concept,"Define normalization as transforming features to be on a similar scale;Explain that normalization helps models converge more quickly during training by preventing gradient descent from taking inefficient paths;Understand that normalization helps models learn appropriate weights for each feature by balancing their initial influence;Recognize that normalization helps avoid numerical overflow and the NaN trap when feature values are very high;Identify that features covering distinctly different ranges should be normalized;Understand that without normalization, models may pay too much attention to features with wide ranges and insufficient attention to features with narrow ranges;Recognize that some optimizers (like Adam and Adagrad) reduce sensitivity to scaling differences, but normalization is still recommended",1.0.0,Understanding goals and benefits of normalization: Define normalization as transforming features to be on a similar scale,,2025-12-31,2025-12-31,,Intermediate,
MLWWND_008,Linear Scaling (Min-Max) Normalization,Concept,"Define linear scaling as converting floating-point values from their natural range into a standard range (usually 0 to 1 or -1 to +1);Calculate a min-max scaled value given the original value, minimum, and maximum of the feature;Identify that linear scaling is appropriate when lower and upper bounds of data don't change much over time;Recognize that linear scaling works well when the feature contains few or no extreme outliers;Understand that linear scaling is suitable when the feature is approximately uniformly distributed across its range;Identify that a histogram with roughly even bars indicates uniform distribution suitable for linear scaling;Recognize human age as an example feature suitable for linear scaling due to stable bounds (0-100) and relatively few outliers",1.0.0,Understanding linear scaling (min-max) normalization: Define linear scaling as converting floating-point values from their natural range into a standard range (usually 0 to 1 or -1 to +1),,2025-12-31,2025-12-31,,Intermediate,
MLWWND_009,Z-score (Standardization) Normalization,Concept,"Define a Z-score as the number of standard deviations a value is from the mean;Calculate a z-score given the original value, mean, and standard deviation of the feature;Understand that Z-score scaling represents features by storing their Z-score in the feature vector;Recognize that Z-score scaling is appropriate when data follows a normal distribution or approximately normal distribution;Identify that a bell-shaped histogram with a peak close to the mean indicates data suitable for Z-score scaling;Understand that Z-score scaling can be combined with clipping to handle extreme outliers in otherwise normal distributions;Recognize that Z-score scaling is typically a better normalization choice than linear scaling for most real-world features",1.0.0,Understanding z-score (standardization) normalization: Define a Z-score as the number of standard deviations a value is from the mean,,2025-12-31,2025-12-31,,Intermediate,
MLWWND_010,Log Scaling Normalization,Concept,"Define log scaling as computing the logarithm (usually natural logarithm) of the raw value;Recognize that log scaling is helpful when data conforms to a power law distribution;Identify characteristics of power law distribution: low X values have very high Y values, and high X values have very low Y values;Understand that a heavy tail-shaped distribution with significant skew indicates data suitable for log scaling;Recognize that log scaling requires positive values; use log1p (log(1+x)) when the feature contains zeros;Understand that log scaling cannot be applied to negative values without first shifting the data;Recognize movie ratings as an example of power law distribution where few items have many ratings and most items have few ratings",1.0.0,Understanding log scaling normalization: Define log scaling as computing the logarithm (usually natural logarithm) of the raw value,,2025-12-31,2025-12-31,,Intermediate,
MLWWND_011,Clipping for Handling Extreme Values,Concept,"Define clipping as setting a maximum and/or minimum threshold and capping values that exceed those thresholds;Recognize clipping as a technique to handle features with extreme outliers;Understand that clipping can be used after other forms of normalization (e.g., z-score with clipping);Identify that clipping is appropriate when features contain extreme outliers that could harm model performance;Understand that clipping reduces information loss compared to deleting outlier examples entirely;Recognize that clipping thresholds should be determined based on domain knowledge or statistical analysis of the training data",1.0.0,Understanding clipping for handling extreme values: Define clipping as setting a maximum and/or minimum threshold and capping values that exceed those thresholds,,2025-12-31,2025-12-31,,Intermediate,
MLWWND_012,Robust Scaling Using Median and IQR,Concept,Define robust scaling as scaling based on the median and interquartile range (IQR) rather than mean and standard deviation;Calculate robust scaled values using the formula: (value - median) / IQR;Recognize that robust scaling is less sensitive to outliers than z-score scaling;Identify that robust scaling is appropriate when data contains outliers that you want to preserve but not let dominate the scaling;Understand the relationship between robust scaling and the quartile statistics already computed during data exploration;Recognize when to prefer robust scaling over z-score scaling or clipping for handling outlier-prone features,1.0.0,Understanding robust scaling using median and iqr: Define robust scaling as scaling based on the median and interquartile range (IQR) rather than mean and standard deviation,,2025-12-31,2025-12-31,,Advanced,
MLWWND_013,Preventing Data Leakage in Normalization,Concept,"Understand that normalization parameters (min/max, mean/std, median/IQR) must be computed using only the training set;Recognize that applying the same fitted parameters to validation, test, and inference data prevents data leakage;Identify data leakage as the error of using information from validation or test sets during training;Understand that data leakage leads to overly optimistic performance estimates that don't generalize;Recognize the need to persist normalization parameters for consistent application during model inference;Demonstrate the correct workflow: fit scaler on training data, then transform training, validation, and test data using those parameters",1.0.0,"Understanding preventing data leakage in normalization: Understand that normalization parameters (min/max, mean/std, median/IQR) must be computed using only the training set",,2025-12-31,2025-12-31,,Advanced,
MLWWND_014,Handling Missing Numerical Values,Concept,"Identify missing values in datasets (represented as NaN, null, or special sentinel values);Recognize mean imputation as replacing missing values with the feature's mean from the training set;Recognize median imputation as replacing missing values with the feature's median from the training set;Understand that median imputation is more robust to outliers than mean imputation;Identify constant imputation as replacing missing values with a fixed value (e.g., 0 or -1);Understand that adding a binary 'missingness indicator' feature can help the model learn patterns related to missing data;Recognize that imputation parameters must be computed on training data only to avoid data leakage",1.0.0,"Understanding handling missing numerical values: Identify missing values in datasets (represented as NaN, null, or special sentinel values)",,2025-12-31,2025-12-31,,Advanced,
MLWWND_015,Bucketing (Binning) Numerical Features,Concept,"Define bucketing as converting continuous numerical values into discrete bins or categories;Recognize that bucketing can help models capture non-linear relationships in features;Identify that bucketing can improve model interpretability by creating meaningful ranges;Understand that bucketing results in information loss since exact values within a bin are treated identically;Recognize that choosing bin boundaries can be done through equal-width intervals, equal-frequency (quantile) intervals, or domain knowledge;Identify situations where bucketing is beneficial: when relationships are non-linear or when interpretability is important;Understand the tradeoff: bucketing adds non-linearity for linear models but loses precision",1.0.0,Understanding bucketing (binning) numerical features: Define bucketing as converting continuous numerical values into discrete bins or categories,,2025-12-31,2025-12-31,,Advanced,
MLWWND_016,The NaN Trap and Numerical Stability,Concept,"Define NaN as an abbreviation for 'not a number', representing an undefined or unrepresentable numerical result;Understand that when a value in a model exceeds the floating-point precision limit, the system may set the value to NaN or Inf;Recognize that when one number in the model becomes a NaN, other numbers in the model also eventually become NaN through propagation;Identify common operations that can create NaNs or Infs: overflow, divide-by-zero, log of non-positive values, square root of negative values;Recognize normalization as a preventive technique to avoid the NaN trap when feature values are very high;Understand that exploding gradients during training can lead to numerical overflow and NaN values",1.0.0,"Understanding the nan trap and numerical stability: Define NaN as an abbreviation for 'not a number', representing an undefined or unrepresentable numerical result",,2025-12-31,2025-12-31,,Advanced,
MLWWND_017,Selecting Appropriate Normalization Techniques,Concept,"Understand that the choice of normalization technique depends on the distribution characteristics of the data;Use histogram shape to guide normalization method selection: flat/uniform suggests linear scaling;Use histogram shape to guide normalization method selection: bell-shaped/normal suggests z-score scaling;Use histogram shape to guide normalization method selection: heavy-tailed/skewed suggests log scaling;Use histogram shape to guide normalization method selection: normal with extreme outliers suggests z-score with clipping or robust scaling;Given a feature distribution plot, select an appropriate transformation and justify the choice;Understand that multiple normalization techniques can be combined (e.g., log transform followed by z-score scaling)",1.0.0,Understanding selecting appropriate normalization techniques: Understand that the choice of normalization technique depends on the distribution characteristics of the data,,2025-12-31,2025-12-31,,Advanced,
MLWWND_018,Data Preparation Workflow for ML Projects,Concept,"Recognize that ML practitioners spend far more time evaluating, cleaning, and transforming data than building models;Understand that data preparation involves multiple stages: exploration, cleaning, transformation, and validation;Identify the recommended workflow: visualize data, compute statistics, detect outliers, handle missing values, normalize features;Recognize that data quality directly impacts model performance and prediction quality;Understand that normalization must be applied consistently across training, validation, test, and inference data;Identify that features must be normalized during training and also when making predictions on new data",1.0.0,"Understanding data preparation workflow for ml projects: Recognize that ML practitioners spend far more time evaluating, cleaning, and transforming data than building models",,2025-12-31,2025-12-31,,Specialized,
