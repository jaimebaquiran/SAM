:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
llm_progressive_learning,Large Language Models - Progressive Learning Path,Curriculum,,1.0.0,"De-duplicated comprehensive curriculum on LLM fundamentals organized by difficulty and prerequisites","Synthesized from 15 AI research publications (2019-2025)",2025-08-18,2025-08-18,12,,
LLM_001,Foundations of AI and LLMs,Concept,"Define artificial intelligence, machine learning, and deep learning, distinguishing between narrow AI and general AI with concrete examples from everyday life;Explain how large language models differ from traditional rule-based programming and earlier AI approaches, identifying at least three key advantages and limitations;Describe how AI systems learn from data rather than being explicitly programmed, using simple analogies to explain pattern recognition and prediction;Identify five real-world applications of LLMs across different industries (education, healthcare, business) and explain their transformative potential;Trace the evolution of language models from ELIZA through statistical models to modern transformers, identifying three major milestones and their significance",1.0.0,Understanding the fundamental concepts of artificial intelligence and large language models,,2025-08-18,2025-08-18,,Foundational,[]
LLM_002,Natural Language Processing Fundamentals,Concept,"Explain how computers convert human language into numerical representations through tokenization, demonstrating at least two different tokenization methods with examples;Describe how word embeddings represent meaning as vectors in multidimensional space, illustrating how similar words cluster together;Identify and explain five core NLP tasks (sentiment analysis, named entity recognition, text classification, summarization, translation) with practical examples;Analyze three fundamental challenges computers face when interpreting human language including ambiguity, context-dependency, and figurative language;Demonstrate understanding of how vocabulary size affects model performance and explain the handling of out-of-vocabulary words and rare terms",1.0.0,Understanding the fundamentals of natural language processing and how computers process human language,,2025-08-18,2025-08-18,,Foundational,["LLM_001"]
LLM_003,Neural Networks and Deep Learning Basics,Concept,"Explain how neural networks process information through layers of neurons, weights, and activation functions using a simple visual diagram or analogy;Differentiate between supervised, unsupervised, and reinforcement learning with specific examples and appropriate use cases for each approach;Describe how early neural network layers detect simple patterns while deeper layers build increasingly complex representations;Explain the training process including forward propagation, loss calculation, backpropagation, and gradient descent in accessible terms;Identify why deep learning excels at certain tasks compared to traditional machine learning, providing three specific examples with performance comparisons",1.0.0,Understanding the basics of neural networks and deep learning architectures,,2025-08-18,2025-08-18,,Foundational,["LLM_001"]
LLM_004,LLM Architecture and Technical Components,Concept,"Explain the transformer architecture's key components (self-attention, multi-head attention, feed-forward networks, positional encoding) using clear diagrams and examples;Describe how attention mechanisms allow models to weigh the importance of different words in context, demonstrating with a specific sentence analysis;Compare transformer models to previous approaches (RNNs, LSTMs) highlighting three advantages including parallelization and long-range dependencies;Explain how context window size limits what information an LLM can process and strategies to work within these constraints;Trace the flow of information through a transformer from input tokens through embedding, attention layers, to output predictions",1.0.0,Understanding the technical architecture and components of large language models,,2025-08-18,2025-08-18,,Foundational,["LLM_002","LLM_003"]
LLM_005,LLM Development Pipeline: Training Fine-tuning and Adaptation,Concept,"Differentiate between pre-training, supervised fine-tuning, instruction tuning, and reinforcement learning from human feedback (RLHF) with specific examples;Explain how models learn language patterns from massive text corpora during pre-training using next-token prediction and masked language modeling;Design a fine-tuning approach for adapting a general LLM to a specific domain, including data requirements and evaluation strategies;Analyze the computational resources and timeframes required for training models of different sizes (1B, 10B, 100B+ parameters);Compare full fine-tuning with parameter-efficient methods (LoRA, adapters, prompt tuning) including trade-offs and appropriate use cases",1.0.0,Understanding the complete development pipeline for training and adapting large language models,,2025-08-18,2025-08-18,,Intermediate,["LLM_004"]
LLM_006,Model Types and Architectures,Concept,"Compare encoder-only (BERT), decoder-only (GPT), and encoder-decoder (T5) architectures, identifying optimal use cases for each type;Explain multimodal models that process text, images, and audio, describing how different modalities are aligned and integrated;Differentiate between foundation models, domain-specific models, and distilled models in terms of capabilities, size, and deployment requirements;Analyze emerging architectures including retrieval-augmented generation (RAG) and tool-using models, explaining how they address traditional LLM limitations;Evaluate trade-offs between model size, inference speed, and task performance when selecting models for specific applications",1.0.0,Understanding different model types and architectures in the LLM ecosystem,,2025-08-18,2025-08-18,,Intermediate,["LLM_004"]
LLM_007,Effective LLM Interaction and Prompt Engineering,Concept,"Demonstrate mastery of five prompting techniques (zero-shot, few-shot, chain-of-thought, role-playing, structured output) with before/after examples;Design effective prompts that include context, constraints, output format specifications, and evaluation criteria for three different task types;Implement strategies to reduce hallucinations and improve factual accuracy through prompt design, including verification steps and confidence indicators;Analyze how prompt structure, word choice, and example selection dramatically affect model outputs using controlled experiments;Create a prompt optimization framework that iteratively improves results through systematic testing and refinement",1.0.0,Understanding effective interaction techniques and prompt engineering for optimal LLM performance,,2025-08-18,2025-08-18,,Intermediate,["LLM_005","LLM_006"]
LLM_008,Evaluating and Validating LLM Performance,Concept,"Apply appropriate evaluation metrics (perplexity, BLEU, ROUGE, F1, accuracy) to different LLM tasks, explaining when each metric is most suitable;Design comprehensive evaluation protocols combining automated metrics with human evaluation for reliability, factuality, and helpfulness;Implement testing strategies including cross-validation, held-out test sets, and adversarial testing to ensure model robustness;Analyze model performance across different demographic groups and domains to identify potential biases and fairness issues;Compare benchmark performance meaningfully, understanding the limitations of standard benchmarks and the importance of task-specific evaluation",1.0.0,Understanding methods for evaluating and validating LLM performance and reliability,,2025-08-18,2025-08-18,,Intermediate,["LLM_007"]
LLM_009,Understanding LLM Limitations and Failure Modes,Concept,"Identify and explain five common failure modes including hallucinations, reasoning errors, factual inaccuracies, temporal confusion, and consistency issues;Analyze why LLMs generate confident but incorrect information, explaining the difference between linguistic competence and true understanding;Demonstrate how context window limitations, training data cutoffs, and lack of real-time information affect model capabilities;Design mitigation strategies for each failure mode including prompt engineering, external validation, and human oversight protocols;Evaluate an LLM's robustness against adversarial inputs, prompt injection attacks, and manipulation attempts with specific examples",1.0.0,Understanding the limitations and failure modes of large language models,,2025-08-18,2025-08-18,,Advanced,["LLM_007","LLM_008"]
LLM_010,Data and Training Considerations,Concept,"Analyze how training data quality, diversity, and scale affect model capabilities, providing specific examples of data-related failures;Identify potential privacy risks including model memorization of sensitive information and strategies for privacy-preserving training;Explain the environmental and computational costs of training large models, comparing different approaches to improve efficiency;Design data curation pipelines that balance representation, quality, and safety while addressing potential biases in data collection;Evaluate the trade-offs between using public web data, curated datasets, and synthetic data for training, including legal and ethical considerations",1.0.0,Understanding data requirements and considerations for training large language models,,2025-08-18,2025-08-18,,Advanced,["LLM_005","LLM_009"]
LLM_011,Ethical AI: Bias Privacy and Responsible Development,Concept,"Identify and analyze five types of bias in LLMs (social, cultural, gender, racial, linguistic) with specific examples and measurement techniques;Apply ethical frameworks (beneficence, non-maleficence, autonomy, justice) to evaluate LLM applications in high-stakes domains;Design comprehensive bias mitigation strategies spanning data curation, model training, evaluation, and deployment phases;Analyze privacy implications of LLM development and deployment, proposing specific technical and policy safeguards;Develop responsible AI guidelines addressing transparency, accountability, fairness, and human oversight for a specific use case",1.0.0,Understanding ethical considerations and responsible development practices for LLMs,,2025-08-18,2025-08-18,,Specialized,["LLM_009","LLM_010"]
LLM_012,AI Governance and Implementation,Concept,"Compare international AI governance approaches and regulations (EU AI Act, US frameworks, China regulations) and their impact on LLM development;Design organizational readiness assessments and implementation strategies for deploying LLMs in production environments;Analyze workforce impacts including job displacement, augmentation, and new skill requirements, proposing adaptation strategies;Develop comprehensive safety protocols including monitoring, incident response, and continuous improvement processes;Evaluate liability and accountability frameworks for LLM-assisted decision-making in regulated industries like healthcare and finance",1.0.0,Understanding governance frameworks and implementation strategies for LLMs in organizations,,2025-08-18,2025-08-18,,Specialized,["LLM_011"]