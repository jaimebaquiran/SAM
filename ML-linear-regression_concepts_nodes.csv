:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
mlcc_lr_v1,ML - Linear Regression,Curriculum,,1.0.0,Converge on the core concepts of linear regression for machine learning,,2026-01-06,2026-01-06,20,,
MLCCLR_001,Supervised Learning and Linear Regression Fundamentals,Concept,"Define supervised learning as learning from labeled examples where both input features and output labels are provided;Explain that linear regression is a supervised learning technique used to find relationships between features and continuous numerical labels;Identify the components of a dataset: examples, features, and labels;Recognize real-world applications where linear regression predicts continuous outputs, such as predicting car fuel efficiency from weight;Distinguish between training phase where the model learns from data and inference phase where the model makes predictions on new data;Explain that the goal of linear regression is to find a best fit line or hyperplane through data points",1.0.0,Understanding supervised learning and linear regression fundamentals: Define supervised learning as learning from labeled examples where both input features and output labels are provided,,2026-01-06,2026-01-06,,Foundational,
MLCCLR_002,Linear Regression Equation Components,Concept,"Write the linear regression equation in ML notation as y-prime equals b plus w times x;Define the predicted label y-prime as the continuous numerical output produced by the model;Explain that bias b represents the y-intercept and indicates the predicted value when all features equal zero;Explain that weight w represents the slope and indicates how much the prediction changes for each unit change in the feature;Identify the feature x as the input value used to make predictions;Recognize that bias and weight are parameters learned during training, while features are provided input values",1.0.0,Understanding linear regression equation components: Write the linear regression equation in ML notation as y-prime equals b plus w times x,,2026-01-06,2026-01-06,,Foundational,
MLCCLR_003,Multiple Feature Linear Models,Concept,"Extend the basic equation to multiple features with separate weights: y-prime equals b plus w1x1 plus w2x2 plus w3x3 and so on;Identify examples of multiple features for predicting car fuel efficiency: engine displacement, acceleration, number of cylinders, and horsepower;Recognize that each feature in a multi-feature model has its own weight parameter that is learned independently;Explain that adding more relevant features can improve model predictions by capturing additional relationships",1.0.0,Understanding multiple feature linear models: Extend the basic equation to multiple features with separate weights: y-prime equals b plus w1x1 plus w2x2 plus w3x3 and so on,,2026-01-06,2026-01-06,,Foundational,
MLCCLR_004,Making Predictions with Trained Models,Concept,"Apply a trained linear regression equation with known weight and bias values to calculate predictions for new feature values;Calculate a prediction by multiplying each feature value by its corresponding weight, summing all products, and adding the bias;Interpret the prediction as the estimated label value for given feature inputs;Use a specific trained model equation such as y-prime equals 34 minus 4.6 times x to predict outcomes for concrete feature values",1.0.0,Understanding making predictions with trained models: Apply a trained linear regression equation with known weight and bias values to calculate predictions for new feature values,,2026-01-06,2026-01-06,,Foundational,
MLCCLR_005,Interpreting Model Parameters,Concept,"Interpret a positive weight as indicating that the label increases as the feature increases;Interpret a negative weight as indicating that the label decreases as the feature increases;Explain that the magnitude of a weight indicates the strength of the relationship between that feature and the label;Interpret the bias as the predicted label value when all features equal zero, noting when this interpretation is or is not meaningful;Compare weights across features to identify which features have stronger influences on predictions",1.0.0,Understanding interpreting model parameters: Interpret a positive weight as indicating that the label increases as the feature increases,,2026-01-06,2026-01-06,,Foundational,
MLCCLR_006,Residuals and Prediction Errors,Concept,Define a residual as the difference between the actual label value and the predicted value for a single example;Calculate residuals using the formula: residual equals actual value minus predicted value;Recognize that residuals can be positive when predictions are too low or negative when predictions are too high;Explain that the sign of residuals indicates the direction of error while the magnitude indicates the size of error;Understand that analyzing residuals helps assess how well a model fits the training data,1.0.0,Understanding residuals and prediction errors: Define a residual as the difference between the actual label value and the predicted value for a single example,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_007,Loss Function Concept and Purpose,Concept,"Define loss as a numerical metric that quantifies how incorrect a model's predictions are on a single example;Explain that loss measures the distance between predictions and actual labels, removing the sign to focus on magnitude;State that the training objective is to find parameters that minimize the average loss across all training examples;Recognize that loss is calculated per example but the training objective uses average loss over the entire dataset;Explain why loss calculations remove the sign: to treat overestimates and underestimates equally in terms of magnitude",1.0.0,Understanding loss function concept and purpose: Define loss as a numerical metric that quantifies how incorrect a model's predictions are on a single example,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_008,Loss Calculation Methods,Concept,"Identify two main methods to remove sign from errors: taking absolute value or squaring the difference;Define L1 loss as the sum of absolute values of differences between predicted and actual values;Define Mean Absolute Error MAE as the average of L1 losses across N examples;Define L2 loss as the sum of squared differences between predicted and actual values;Define Mean Squared Error MSE as the average of L2 losses across N examples;Define Root Mean Squared Error RMSE as the square root of MSE, returning error to the original label units;Calculate L2 loss for a single example given a prediction and actual value using the squaring formula",1.0.0,Understanding loss calculation methods: Identify two main methods to remove sign from errors: taking absolute value or squaring the difference,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_009,Comparing and Choosing Loss Functions,Concept,Explain that squaring amplifies large differences and reduces small differences below 1;Define an outlier as a data point with feature values or prediction errors that fall outside the typical range;Recognize that MSE penalizes large errors much more heavily than MAE due to squaring;Explain that models trained with MSE are more influenced by outliers while MAE-trained models are less affected;Choose MSE when large errors should be heavily penalized or when outliers represent important data variance;Choose MAE when the dataset contains significant outliers that should not dominate model training;Recognize that MAE and RMSE are more interpretable because they measure error in the same units as the label,1.0.0,Understanding comparing and choosing loss functions: Explain that squaring amplifies large differences and reduces small differences below 1,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_010,Training Objective and Empirical Risk Minimization,Concept,State the training objective as finding parameters that minimize the average loss over all training examples;Explain that minimizing average training loss is called empirical risk minimization;Recognize that the objective function combines individual example losses into a single value to optimize;Understand that better models achieve lower average loss on the training dataset;Distinguish between loss on individual examples and the overall training objective across the dataset,1.0.0,Understanding training objective and empirical risk minimization: State the training objective as finding parameters that minimize the average loss over all training examples,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_011,Gradient Descent Algorithm Fundamentals,Concept,"Define gradient descent as an iterative optimization algorithm that finds optimal weights and bias by minimizing the loss function;Describe that training begins with randomly initialized weights and bias values close to zero;List the iterative steps: calculate loss on training data, compute gradients, update parameters in the direction that reduces loss, repeat;Explain that gradient descent makes small adjustments to parameters repeatedly until the loss stops decreasing significantly;Recognize that gradient descent is a general mathematical optimization technique applicable to many machine learning algorithms",1.0.0,Understanding gradient descent algorithm fundamentals: Define gradient descent as an iterative optimization algorithm that finds optimal weights and bias by minimizing the loss function,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_012,Gradient Calculation and Parameter Updates,Concept,Explain that the gradient represents the direction of steepest increase in the loss function;Describe that gradients are calculated by taking partial derivatives of the loss function with respect to each parameter;Recognize that parameters are updated by moving in the opposite direction of the gradient to reduce loss;Understand the conceptual update rule: new parameter value equals old value minus learning rate times gradient;Explain that both weight and bias are updated simultaneously during each iteration using their respective gradients,1.0.0,Understanding gradient calculation and parameter updates: Explain that the gradient represents the direction of steepest increase in the loss function,,2026-01-06,2026-01-06,,Intermediate,
MLCCLR_013,Learning Rate Hyperparameter,Concept,"Define learning rate as the hyperparameter controlling the step size of parameter updates during gradient descent;Recognize that learning rate is the multiplier applied to gradients to determine how much to adjust parameters;Explain that a learning rate that is too large can cause training to diverge or oscillate without converging;Explain that a learning rate that is too small causes training to converge very slowly, requiring many iterations;Understand that choosing an appropriate learning rate is essential for efficient and successful model training",1.0.0,Understanding learning rate hyperparameter: Define learning rate as the hyperparameter controlling the step size of parameter updates during gradient descent,,2026-01-06,2026-01-06,,Advanced,
MLCCLR_014,"Batch, Mini-Batch, and Stochastic Gradient Descent",Concept,Define batch gradient descent as computing gradients using all training examples before each parameter update;Define stochastic gradient descent SGD as computing gradients using one randomly selected example before each update;Define mini-batch gradient descent as computing gradients using a small random subset of examples before each update;Explain that batch gradient descent produces smooth convergence but is computationally expensive for large datasets;Explain that SGD and mini-batch methods are faster per iteration but produce noisier loss curves;Recognize that mini-batch gradient descent balances computational efficiency with stable convergence,1.0.0,"Understanding batch, mini-batch, and stochastic gradient descent: Define batch gradient descent as computing gradients using all training examples before each parameter update",,2026-01-06,2026-01-06,,Advanced,
MLCCLR_015,Model Convergence and Stopping Criteria,Concept,"Define convergence as the state when additional training iterations produce negligible reduction in loss;Explain that a converged model has found parameter values that achieve a local or global minimum of the loss function;Recognize common stopping criteria: loss improvement below threshold, maximum iterations reached, or validation performance plateaus;Understand that convergence does not guarantee the absolute lowest possible loss but rather a stable minimum given the optimization process;Distinguish between stopping due to convergence versus stopping early to prevent overfitting",1.0.0,Understanding model convergence and stopping criteria: Define convergence as the state when additional training iterations produce negligible reduction in loss,,2026-01-06,2026-01-06,,Advanced,
MLCCLR_016,Loss Curves and Training Diagnostics,Concept,Describe loss curves as plots showing how training loss changes over iterations or epochs;Interpret a steadily decreasing loss curve as indicating successful training progress;Identify a loss curve that increases or diverges as indicating a learning rate that is too high;Identify a loss curve that decreases very slowly as indicating a learning rate that may be too low;Recognize that noisy or oscillating loss curves are typical when using stochastic or mini-batch gradient descent;Use loss curves to determine when a model has converged and training can be stopped,1.0.0,Understanding loss curves and training diagnostics: Describe loss curves as plots showing how training loss changes over iterations or epochs,,2026-01-06,2026-01-06,,Advanced,
MLCCLR_017,Convex Loss Functions in Linear Regression,Concept,"State that linear regression with squared loss MSE has a convex loss function shaped like a single bowl;Explain that convex loss functions have the property that any local minimum is also the global minimum;Recognize that for convex objectives, gradient descent with an appropriate learning rate will converge to the global minimum;Understand that convexity guarantees an optimal solution exists and can be found, unlike non-convex problems with multiple local minima;Note that convexity applies specifically to the choice of squared loss and may not hold for other loss functions or model types",1.0.0,Understanding convex loss functions in linear regression: State that linear regression with squared loss MSE has a convex loss function shaped like a single bowl,,2026-01-06,2026-01-06,,Advanced,
MLCCLR_018,Feature Scaling and Normalization,Concept,"Explain that features with different scales or ranges can cause gradient descent to converge slowly or inefficiently;Define feature scaling as transforming features to have similar ranges, typically between 0 and 1 or with mean 0 and standard deviation 1;Recognize that normalization helps gradient descent converge faster by creating a more spherical loss surface;Identify when feature scaling is important: when features have vastly different ranges such as age in years versus income in dollars;Apply the concept that scaled features lead to more balanced gradient magnitudes across parameters",1.0.0,Understanding feature scaling and normalization: Explain that features with different scales or ranges can cause gradient descent to converge slowly or inefficiently,,2026-01-06,2026-01-06,,Advanced,
MLCCLR_019,Generalization and Overfitting,Concept,"Define generalization as a model's ability to make accurate predictions on new unseen data;Explain that minimizing training loss does not guarantee good performance on new data;Define overfitting as when a model learns training data too well, including noise, and performs poorly on new data;Recognize that the goal is not just low training loss but good performance on validation and test data;Understand the importance of splitting data into training, validation, and test sets to assess generalization;Identify overfitting by comparing training loss that continues decreasing while validation loss increases or plateaus",1.0.0,Understanding generalization and overfitting: Define generalization as a model's ability to make accurate predictions on new unseen data,,2026-01-06,2026-01-06,,Specialized,
MLCCLR_020,Regularization in Linear Regression,Concept,"Define regularization as a technique that adds a penalty term to the loss function to discourage complex models;Explain that L2 regularization, also called ridge regression, adds a penalty proportional to the sum of squared weights;Recognize that regularization helps prevent overfitting by keeping weight values smaller;Understand that regularization introduces a hyperparameter that controls the strength of the penalty;Explain the trade-off: stronger regularization reduces overfitting but may increase training loss and underfitting;Identify that regularization penalizes weights but typically does not penalize the bias term",1.0.0,Understanding regularization in linear regression: Define regularization as a technique that adds a penalty term to the loss function to discourage complex models,,2026-01-06,2026-01-06,,Specialized,
