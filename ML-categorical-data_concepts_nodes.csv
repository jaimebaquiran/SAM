:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
ML_working_with_categorical_data_v1,Machine Learning - Working with categorical data,Curriculum,,1.0.0,Discern when to wield categories with numbers,,2026-01-03,2026-01-03,9,,
MLWWCD_001,Categorical vs Numerical Data,Concept,"Identify that categorical data has a specific set of possible values (e.g., species of animals, street names, email spam status, house colors);Distinguish that true numerical data can be meaningfully multiplied (e.g., a 200 square meter house should be roughly twice as valuable as a 100 square meter house);Recognize that integer values like postal codes should often be represented as categorical rather than numerical data to avoid implying false numeric relationships;Explain why representing postal codes numerically would incorrectly suggest that postal code 20004 is twice as large a signal as postal code 10002;Distinguish between ordinal categorical features (e.g., 'small', 'medium', 'large' with inherent order) and nominal categorical features (e.g., colors with no inherent order);Understand that categorical representation allows the model to weight each individual category separately rather than finding numeric relationships between them;Given a dataset schema with mixed data types, identify which columns should be treated as categorical versus numerical and justify the choice",1.0.0,"Understanding categorical vs numerical data: Identify that categorical data has a specific set of possible values (e.g., species of animals, street names, email spam status, house colors)",,2026-01-03,2026-01-03,,Foundational,
MLWWCD_002,Vocabulary and One-Hot Encoding,Concept,"Define vocabulary as the complete set of allowed categorical values for a feature, and explain that vocabulary encoding treats each value as a separate feature;Identify low-dimensional categorical features as those with a small number of categories (typically 2-10 categories like days of week or seasons);Explain that models can only manipulate floating-point numbers and cannot train directly on raw strings, requiring conversion to numeric representations;Define one-hot encoding as a method where each category is represented by a vector of N elements (where N is the number of categories), with exactly one element having value 1.0 and all others having value 0.0;Apply the complete conversion process: string → unique index number → one-hot vector (e.g., 'Red' → index 0 → [1,0,0,0,0,0,0,0]);Recognize that for binary categorical features (two categories), a single 0/1 value can be used instead of a two-element one-hot vector;Understand that for linear models with an intercept term, using k−1 encoding (dropping one category) avoids multicollinearity, though modern frameworks often handle full one-hot encoding automatically;Distinguish between one-hot encoding (only one value is 1.0) and multi-hot encoding (multiple values can be 1.0 for multi-label features like movie genres or product tags);Identify how to handle missing values in categorical features by either creating an explicit 'missing' category or using imputation strategies",1.0.0,"Understanding vocabulary and one-hot encoding: Define vocabulary as the complete set of allowed categorical values for a feature, and explain that vocabulary encoding treats each value as a separate feature",,2026-01-03,2026-01-03,,Foundational,
MLWWCD_003,Sparse Representation,Concept,"Define a sparse feature as one whose values are predominantly zero or empty;Explain that sparse representation stores only the position(s) of nonzero values rather than the entire vector (e.g., [0,0,1,0,0,0,0,0] becomes '2');Convert one-hot vectors to sparse representation by identifying the index of the 1.0 value;Recognize that sparse representation consumes far less memory than storing full one-hot vectors, especially for high-dimensional features;Understand that the model's mathematical operations are equivalent to operating on the full one-hot vector, though modern implementations use sparse tensor operations for efficiency;Apply sparse representation to multi-hot encoding by storing positions of all nonzero elements (e.g., a movie with genres 'Action' and 'Comedy' at positions 2 and 5 is represented as '2, 5');Explain when sparsity benefits break down (e.g., when many categories are active simultaneously, making the representation less sparse)",1.0.0,Understanding sparse representation: Define a sparse feature as one whose values are predominantly zero or empty,,2026-01-03,2026-01-03,,Foundational,
MLWWCD_004,Out-of-Vocabulary (OOV) and Unknown Categories,Concept,"Recognize that categorical data contains outliers similar to numerical data (e.g., rarely used car colors like 'Mauve' or 'Avocado');Define out-of-vocabulary (OOV) as a catch-all category that lumps rare or outlier categories together;Apply frequency-based thresholding to identify rare categories during training (e.g., keep only categories appearing at least 10 times, map rest to OOV);Distinguish between rare-but-known categories during training (mapped to OOV based on frequency threshold) and completely unknown categories at inference time (new values never seen during training);Explain how to handle unknown categories at inference by mapping them to the OOV bucket or using hashing to assign them to existing buckets;Understand that the model learns a single weight for the entire OOV bucket rather than separate weights for each rare category;Given a categorical feature distribution, choose an appropriate frequency threshold and explain the tradeoff between vocabulary size and information loss",1.0.0,"Understanding out-of-vocabulary (oov) and unknown categories: Recognize that categorical data contains outliers similar to numerical data (e.g., rarely used car colors like 'Mauve' or 'Avocado')",,2026-01-03,2026-01-03,,Intermediate,
MLWWCD_005,Embeddings for High-Dimensional Categorical Features,Concept,"Identify high-dimensional categorical features as those with many categories (e.g., ~500,000 English words, ~42,000 US postal codes, ~850,000 German surnames);Recognize that one-hot encoding is usually a poor choice for high-dimensional features due to memory consumption and computational cost;Define an embedding as a learned dense vector representation that maps each category to a lower-dimensional continuous space (e.g., mapping 50,000 words to 128-dimensional vectors);Explain that embeddings are learned during model training, with the model adjusting embedding values to minimize prediction error;Understand that embeddings benefit models by enabling faster training, lower inference latency, and the ability to capture semantic relationships between categories;Compare embeddings to one-hot encoding: embeddings are dense (most values nonzero) and low-dimensional, while one-hot vectors are sparse and high-dimensional;Given a high-dimensional categorical feature, choose an appropriate embedding dimension (typically 10-300) based on the number of categories and available training data",1.0.0,"Understanding embeddings for high-dimensional categorical features: Identify high-dimensional categorical features as those with many categories (e.g., ~500,000 English words, ~42,000 US postal codes, ~850,000 German surnames)",,2026-01-03,2026-01-03,,Intermediate,
MLWWCD_006,Feature Hashing,Concept,"Define feature hashing (the hashing trick) as a method that maps categories to a fixed number of buckets using a hash function, reducing dimensionality;Apply the hashing process: set N bins, choose a hash function, pass each category through the function to get a hash value, assign to bin using modulo N (e.g., hash('Blue') = 89237, bin = 89237 % 100 = 37);Recognize that after hashing assigns categories to bins, a one-hot encoding is created for those N bins rather than the original number of categories;Define hash collision as when multiple different categories map to the same bin, and explain that collisions introduce noise but are often acceptable;Understand the tradeoff in choosing N: larger N reduces collisions but increases dimensionality; smaller N saves memory but increases collision rate;Compare hashing to embeddings: hashing is deterministic and requires no training, while embeddings are learned and can capture semantic relationships;Explain when to use hashing: when you need a simple, fixed-size representation without training overhead, or when categories are extremely high-dimensional and constantly changing",1.0.0,"Understanding feature hashing: Define feature hashing (the hashing trick) as a method that maps categories to a fixed number of buckets using a hash function, reducing dimensionality",,2026-01-03,2026-01-03,,Intermediate,
MLWWCD_007,High-Cardinality Features and Leakage Risks,Concept,"Define cardinality as the number of unique values in a categorical feature;Identify extremely high-cardinality features like user_id, device_id, or transaction_id that may have millions of unique values;Recognize that using high-cardinality ID features directly can cause the model to memorize individual examples rather than learn generalizable patterns;Explain data leakage risk: when a high-cardinality feature perfectly or nearly perfectly predicts the target, the model may perform well on training data but fail on new data;Apply strategies to handle high-cardinality features: use embeddings, aggregate statistics (e.g., user's average purchase amount), or exclude the feature entirely;Understand when high-cardinality features are appropriate: when you have sufficient data per category and the feature genuinely contains predictive signal beyond memorization",1.0.0,Understanding high-cardinality features and leakage risks: Define cardinality as the number of unique values in a categorical feature,,2026-01-03,2026-01-03,,Advanced,
MLWWCD_008,Data Quality Issues in Categorical Features,Concept,"Distinguish between gold labels (human-labeled data) and silver labels (machine-labeled data), recognizing that neither automatically guarantees high quality;Identify categorical-specific data quality issues: inconsistent casing or whitespace ('Blue' vs ' blue '), synonyms ('NYC' vs 'New York'), typos, and taxonomy drift over time;Define inter-rater agreement as the consistency between different human raters labeling the same example, and recognize that low agreement indicates ambiguous or poorly defined categories;Recognize that poor quality labels (e.g., mislabeling a chihuahua as a muffin) will result in lower quality trained models;Apply data cleaning techniques for categorical features: standardize casing, trim whitespace, map synonyms to canonical values, and validate against expected vocabularies;Understand that machine-labeled data should be checked for accuracy, biases, and violations of common sense or reality before use in training;Implement quality checks: use multiple raters per example, measure inter-rater agreement, sample and manually inspect labels, and monitor for category distribution shifts over time",1.0.0,"Understanding data quality issues in categorical features: Distinguish between gold labels (human-labeled data) and silver labels (machine-labeled data), recognizing that neither automatically guarantees high quality",,2026-01-03,2026-01-03,,Advanced,
MLWWCD_009,Feature Crosses,Concept,"Define feature crosses as synthetic features created by combining (taking the Cartesian product of) two or more categorical or bucketed features;Explain that feature crosses allow linear models to learn nonlinear relationships by explicitly encoding interactions between features;Demonstrate why linear models need crosses: show that a linear model cannot represent XOR-like interactions (e.g., 'smooth AND opposite' being predictive) without feature crosses;Apply the feature cross calculation where each crossed feature is the product of base feature values (e.g., if edges=[1,0,0] and arrangement=[0,1], then Smooth_Opposite = edges[0] * arrangement[1] = 0);Construct a feature cross from two categorical features: crossing a 3-category feature with a 2-category feature produces 6 new crossed features (3 × 2);Recognize that crossing sparse features produces even sparser features (e.g., 100-element feature crossed with 200-element feature yields 20,000-element feature, mostly zeros);Understand that feature crosses can cause dimensionality explosion and overfitting, requiring regularization, feature hashing, or careful selection based on domain knowledge;Recognize that neural networks can automatically discover useful feature interactions during training without manual feature crossing, making crosses less critical for deep learning models",1.0.0,Understanding feature crosses: Define feature crosses as synthetic features created by combining (taking the Cartesian product of) two or more categorical or bucketed features,,2026-01-03,2026-01-03,,Advanced,
