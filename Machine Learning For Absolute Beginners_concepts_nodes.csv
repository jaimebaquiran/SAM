:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
ML_002,Machine Learning for Absolute Beginners,Curriculum,,1.0.0,An introduction to machine learning for beginners,,2025-12-22,2025-12-22,10,,
MLBEG_001,What Machine Learning Is and When to Use It,Concept,"Explain Arthur Samuel's definition: machine learning gives computers the ability to learn from data without being explicitly programmed for each specific task;Describe a machine learning model as a mathematical function that maps input features to output predictions;Contrast traditional rule-based programming (explicit commands → actions) with machine learning (data → learned patterns → predictions);Identify appropriate use cases for machine learning: problems with patterns in data, tasks too complex for manual rules, or situations requiring adaptation to new data;Explain the fundamental ML workflow: collect data → train model on patterns → use model to make predictions on new data;Recognize when machine learning is NOT appropriate: insufficient data, need for explainability in high-stakes decisions, or when simple rules suffice",1.0.0,Understanding what machine learning is and when to use it: Explain Arthur Samuel's definition: machine learning gives computers the ability to learn from data without being explicitly programmed for each specific task,,2025-12-22,2025-12-22,,Foundational,
MLBEG_002,Supervised Learning: Problem Types and Core Vocabulary,Concept,"Define supervised learning as learning from labeled examples where both input features and correct output labels are provided during training;Distinguish classification (predicting categories: spam/not spam, cat/dog/bird) from regression (predicting continuous numbers: house price, temperature);Explain the core data vocabulary: dataset (collection of examples), features (input variables/attributes), label/target (output variable to predict), prediction (model's output);Describe what constitutes a single training example: one row with feature values and its corresponding true label;Given a real-world problem (e.g., predicting customer churn, estimating delivery time), identify whether it is classification or regression and list potential features and the target variable;Explain ground truth as the actual correct labels in your dataset, used to measure how well your model performs",1.0.0,Understanding supervised learning: problem types and core vocabulary: Define supervised learning as learning from labeled examples where both input features and correct output labels are provided during training,,2025-12-22,2025-12-22,,Foundational,
MLBEG_003,Train-Validation-Test Split and Preventing Data Leakage,Concept,"Explain why data must be split into separate sets: training set (to fit the model), validation set (to tune and compare models), and test set (for final unbiased evaluation);Describe the purpose of each set: training data teaches the model patterns, validation data helps select the best model and hyperparameters, test data estimates real-world performance;Explain the critical rule: test data must be held out and used only once at the end, never during model development;Define data leakage as information from outside the training set improperly influencing the model, leading to overly optimistic performance estimates;Identify examples of data leakage: using test data during training, including future information in features, or preprocessing before splitting;Given a dataset scenario, propose an appropriate split ratio (e.g., 70% train, 15% validation, 15% test) and justify why the test set must remain untouched until final evaluation",1.0.0,"Understanding train-validation-test split and preventing data leakage: Explain why data must be split into separate sets: training set (to fit the model), validation set (to tune and compare models), and test set (for final unbiased evaluation)",,2025-12-22,2025-12-22,,Foundational,
MLBEG_004,"Overfitting, Generalization, and Model Performance",Concept,"Define overfitting as when a model learns training data too well, including noise and specific details, causing poor performance on new unseen data;Explain generalization as a model's ability to perform well on new data it has never seen before, which is the true goal of machine learning;Contrast training performance (how well the model fits training data) with test performance (how well it generalizes to new data);Describe the warning signs of overfitting: high training accuracy but significantly lower test accuracy;Explain why memorization fails: a model that simply remembers training examples cannot handle new situations or variations;Identify strategies to reduce overfitting: collect more training data, use simpler models, apply regularization techniques, or reduce the number of features",1.0.0,"Understanding overfitting, generalization, and model performance: Define overfitting as when a model learns training data too well, including noise and specific details, causing poor performance on new unseen data",,2025-12-22,2025-12-22,,Intermediate,
MLBEG_005,Evaluation Metrics: Measuring Model Performance,Concept,"Explain why accuracy alone can be misleading, especially with imbalanced datasets (e.g., 95% non-spam emails means always predicting 'not spam' achieves 95% accuracy but catches no spam);Describe the confusion matrix components for classification: true positives, true negatives, false positives (Type I error), and false negatives (Type II error);Define precision (of positive predictions, how many are correct) and recall (of actual positives, how many did we find), and explain the tradeoff between them;Explain regression metrics conceptually: MAE (mean absolute error) measures average prediction error magnitude, MSE (mean squared error) penalizes large errors more heavily;Given a problem context (e.g., cancer diagnosis, spam detection, house price prediction), select an appropriate primary metric and justify the choice;Explain the importance of baseline models: always compare your model to a simple baseline (e.g., predicting the most common class, or predicting the mean value) to verify your model adds value",1.0.0,"Understanding evaluation metrics: measuring model performance: Explain why accuracy alone can be misleading, especially with imbalanced datasets (e.g., 95% non-spam emails means always predicting 'not spam' achieves 95% accuracy but catches no spam)",,2025-12-22,2025-12-22,,Intermediate,
MLBEG_006,The Iterative ML Development Cycle,Concept,"Describe the iterative nature of ML development: train model → evaluate on validation set → analyze errors → improve → repeat;Identify the main levers for improvement: (1) get more or better quality data, (2) engineer better features, (3) try different model types, (4) tune hyperparameters, (5) adjust decision thresholds;Explain the difference between parameters (learned automatically from data, like weights in a model) and hyperparameters (settings chosen by the practitioner, like k in k-nearest neighbors or tree depth);Describe how models learn: by optimizing parameters to minimize a loss function that measures prediction errors on training data;Explain error analysis: examining which examples the model gets wrong to identify patterns and guide improvements;Understand that model development is experimental: trying different approaches, measuring results on validation data, and iterating based on evidence rather than guessing",1.0.0,Understanding the iterative ml development cycle: Describe the iterative nature of ML development: train model → evaluate on validation set → analyze errors → improve → repeat,,2025-12-22,2025-12-22,,Intermediate,
MLBEG_007,Data Preparation Essentials,Concept,"Explain why raw data usually requires preprocessing before machine learning: missing values, inconsistent formats, different scales, and categorical variables;Describe strategies for handling missing data: removal (if few), imputation with mean/median/mode, or using indicators that a value was missing;Explain categorical encoding: converting non-numeric categories (colors, countries) into numeric format, such as one-hot encoding creating binary columns for each category;Describe feature scaling: standardizing numeric features to similar ranges so no single feature dominates due to its scale (e.g., age 20-80 vs income 20000-80000);Explain the critical rule: fit preprocessing steps (like calculating means for scaling) only on training data, then apply the same transformation to validation and test data to prevent leakage;Recognize that data quality directly impacts model performance: garbage in, garbage out",1.0.0,"Understanding data preparation essentials: Explain why raw data usually requires preprocessing before machine learning: missing values, inconsistent formats, different scales, and categorical variables",,2025-12-22,2025-12-22,,Advanced,
MLBEG_008,Introduction to Model Families and Algorithm Selection,Concept,"Identify that different model families have different strengths: linear models (simple, interpretable), tree-based models (handle non-linear patterns, feature interactions), k-nearest neighbors (simple, instance-based);Explain linear regression conceptually: finds the best straight line (or plane in multiple dimensions) through the data to predict continuous values;Explain decision trees conceptually: creates a flowchart of yes/no questions about features to partition data and make predictions;Describe k-nearest neighbors conceptually: predicts based on the k most similar training examples (nearest neighbors in feature space);Understand that algorithm selection depends on: problem type (classification/regression), data size, interpretability needs, and performance requirements;Recognize that no single algorithm is best for all problems; experimentation and validation-set comparison guide selection",1.0.0,"Understanding introduction to model families and algorithm selection: Identify that different model families have different strengths: linear models (simple, interpretable), tree-based models (handle non-linear patterns, feature interactions), k-nearest neighbors (simple, instance-based)",,2025-12-22,2025-12-22,,Advanced,
MLBEG_009,"Real-World Applications, Limitations, and Ethical Considerations",Concept,"Identify diverse ML applications: recommendation systems (YouTube, Netflix), fraud detection, medical diagnosis, autonomous vehicles, spam filtering, and predictive maintenance;Explain that ML finds correlations in data but does not establish causation; correlation does not imply one variable causes another;Recognize ML limitations: requires substantial quality data, can perpetuate biases present in training data, may fail on edge cases not represented in training, and lacks common sense reasoning;Describe how biased training data leads to biased models: if historical data reflects discriminatory practices, the model learns and reproduces those biases;Explain basic fairness concerns: models should not discriminate based on protected attributes (race, gender) and predictions should be equitable across groups;Understand privacy considerations: training data may contain sensitive personal information requiring protection, and model predictions can reveal private details;Identify when human oversight is essential: high-stakes decisions (medical, legal, financial) require human judgment, not blind trust in model outputs",1.0.0,"Understanding real-world applications, limitations, and ethical considerations: Identify diverse ML applications: recommendation systems (YouTube, Netflix), fraud detection, medical diagnosis, autonomous vehicles, spam filtering, and predictive maintenance",,2025-12-22,2025-12-22,,Advanced,
MLBEG_010,The Role of Programming and Tools in Machine Learning,Concept,"Explain that programming is essential for ML implementation: loading and manipulating data, configuring and training models, evaluating results, and deploying solutions;Understand that ML libraries (like scikit-learn, TensorFlow, PyTorch) provide pre-built algorithm implementations, eliminating the need to code mathematical equations from scratch;Describe the practitioner's programming tasks: data preprocessing, selecting algorithms, setting hyperparameters, running training loops, and interpreting outputs;Recognize that while AutoML tools can automate some steps, understanding ML fundamentals remains necessary for proper problem formulation, data preparation, evaluation, and validation;Explain that statistical and mathematical foundations (primarily algebra and statistics) underpin ML algorithms, though libraries handle the computational details;Understand that ML engineering involves more than just training models: data pipelines, model monitoring for performance degradation, handling data drift, and maintaining production systems",1.0.0,"Understanding the role of programming and tools in machine learning: Explain that programming is essential for ML implementation: loading and manipulating data, configuring and training models, evaluating results, and deploying solutions",,2025-12-22,2025-12-22,,Specialized,
