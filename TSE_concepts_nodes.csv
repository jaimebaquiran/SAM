:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
tsa_ai_v1,The Scaling Era - An Oral History of AI 2019-2025,Curriculum,,1.0.0,Discuss the history of AI through the lens of The Scaling Era,,2026-01-09,2026-01-09,22,,
TSEAI_001,The Scaling Hypothesis: Core Mechanism and Training Process,Concept,"Explain how LLMs learn by predicting the next token in a document through trillions of token-prediction steps;Describe why LLMs are considered grown rather than designed, unlike traditional software systems;Define pretraining as the process of creating an initial LLM by adjusting weights through exposure to vast amounts of internet text;Explain the distinction between pretraining on general data and post-training for assistant-like behaviors;Describe how the scaling approach of using more data, compute, and parameters led to unexpected capability improvements",1.0.0,Understanding the scaling hypothesis: core mechanism and training process: Explain how LLMs learn by predicting the next token in a document through trillions of token-prediction steps,,2026-01-09,2026-01-09,,Foundational,
TSEAI_002,The Uneven Intelligence Profile of LLMs,Concept,"Identify superhuman capabilities including reading speed, breadth of knowledge, multilingual fluency, and next-word prediction accuracy;Identify subhuman limitations including the large number of examples needed to learn new skills, failure at precise symbol manipulation, and lack of episodic memory;Explain why people tend to either dismiss LLMs as stochastic parrots or overestimate them as human-like intelligence rather than accepting their uneven profile;Describe the metaphor of LLMs as brilliant but somewhat deluded and amnesic coworkers;Contrast the unevenness of LLMs with the familiar unevenness of 20th century computers that were superhumanly fast but unable to understand natural language",1.0.0,"Understanding the uneven intelligence profile of llms: Identify superhuman capabilities including reading speed, breadth of knowledge, multilingual fluency, and next-word prediction accuracy",,2026-01-09,2026-01-09,,Foundational,
TSEAI_003,Multitask Generality and the Breadth of LLM Capabilities,Concept,"Explain how LLMs are massively multitask systems that can be used for diverse functions and often outperform single-task systems;Describe the argument for broad task coverage via next-token prediction and name key limitations or counterarguments;Explain how LLMs function as effective general compression algorithms despite not being intentionally trained for this purpose;Identify how LLMs can speak with grammatically correct language on varied topics in major world languages and write in different literary styles;Provide examples of unexpected capabilities that emerged from scaling, such as a car dealership AI answering advanced mathematics questions",1.0.0,Understanding multitask generality and the breadth of llm capabilities: Explain how LLMs are massively multitask systems that can be used for diverse functions and often outperform single-task systems,,2026-01-09,2026-01-09,,Foundational,
TSEAI_004,"Opacity, Interpretability, and What We Know About LLM Internals",Concept,"Explain why researchers state that we do not fully understand how LLMs work internally despite their impressive capabilities;Describe what interpretability research attempts to uncover, such as circuits and probing techniques, and why this work is challenging;Distinguish between what we know about LLM behavior through observation versus what we understand about their internal mechanisms;Explain the contrast between traditional software where we design every component versus LLMs where capabilities emerge from training;Describe the ongoing scientific effort to develop mechanistic understanding of transformer models",1.0.0,"Understanding opacity, interpretability, and what we know about llm internals: Explain why researchers state that we do not fully understand how LLMs work internally despite their impressive capabilities",,2026-01-09,2026-01-09,,Foundational,
TSEAI_005,"Prompt Sensitivity, Steerability, and Emergent Behaviors",Concept,"Describe how LLMs only act when prompted and their only action is to predict what should come next in a sequence;Demonstrate with examples how framing and prompting changes outputs, such as asking for smarter answers producing more thoughtful replies;Explain plausible reasons for prompt sensitivity including conditioning on instruction-following patterns in training data;Identify the phenomenon where LLMs occasionally generate information that appears novel or not directly present in training data;Describe how prompt engineering became a practical skill for getting desired behaviors from LLMs",1.0.0,"Understanding prompt sensitivity, steerability, and emergent behaviors: Describe how LLMs only act when prompted and their only action is to predict what should come next in a sequence",,2026-01-09,2026-01-09,,Foundational,
TSEAI_006,"Hallucination, Confabulation, and Reliability Challenges",Concept,"Define hallucination as when LLMs generate plausible-sounding but incorrect or fabricated information;Provide examples of out-of-context regurgitation, such as advising someone to eat rocks for nutritional benefits based on misapplied training data;Explain the contrast between traditional rigid logical robots and LLMs that are difficult to prevent from making things up;Describe why people who work with LLMs learn to constantly verify and doubt their output;Identify the challenge of distinguishing between genuine knowledge, memorized patterns, and confabulated information in LLM responses",1.0.0,"Understanding hallucination, confabulation, and reliability challenges: Define hallucination as when LLMs generate plausible-sounding but incorrect or fabricated information",,2026-01-09,2026-01-09,,Foundational,
TSEAI_007,The Transformer Architecture and Self-Attention Mechanism,Concept,"Define a Transformer as a modern neural network architecture notable for its parallel design that processes sequences efficiently;Explain the self-attention mechanism that dynamically assigns varying importance to different parts of input data;Describe how Transformers learn context and relationships in data through the attention mechanism without explicit programming;Identify that leading LLMs including GPT, Claude, and Gemini are based on the Transformer architecture;Explain how Transformers can be trained to emit more than just text tokens, including actions, images, and other modalities",1.0.0,Understanding the transformer architecture and self-attention mechanism: Define a Transformer as a modern neural network architecture notable for its parallel design that processes sequences efficiently,,2026-01-09,2026-01-09,,Intermediate,
TSEAI_008,Post-Training: From Base Models to Assistants,Concept,"Define post-training as the process of adapting pretrained models to be more helpful, harmless, and honest assistants;Explain instruction tuning as training on examples of instructions and desired responses to make models follow user intent;Describe reinforcement learning from human feedback as using human preferences to guide model behavior toward desired outcomes;Distinguish between training on general internet data versus training on curated chat sessions or preference rankings;Explain how post-training makes models more professional, less toxic, and better at following complex instructions",1.0.0,"Understanding post-training: from base models to assistants: Define post-training as the process of adapting pretrained models to be more helpful, harmless, and honest assistants",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_009,"Neural Network Fundamentals: Parameters, Weights, and Learning",Concept,Define a parameter as a numerical value adjusted iteratively during model training to encode patterns learned from data;Explain weights as parameters that define the strength of connections between units in a neural network;Describe the metaphor of weights as synapses in the brain that get strengthened or weakened through experience;Define learning as the process of adjusting weights in a model after it processes data to enable improved predictions on future inputs;Explain using simple examples how patterns like autocomplete learn from repeated exposure to text sequences,1.0.0,"Understanding neural network fundamentals: parameters, weights, and learning: Define a parameter as a numerical value adjusted iteratively during model training to encode patterns learned from data",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_010,"Evaluation Science: Benchmarks, Capabilities Testing, and Red-Teaming",Concept,"Define evaluations as systematic tests designed to measure what LLMs can and cannot do across different domains;Explain why measuring AI progress became its own scientific discipline during the scaling era;Describe the distinction between capability evaluations that test what models can do and reliability evaluations that test consistency;Identify how benchmarks can be gamed or saturated, requiring continuous development of new evaluation methods;Explain red-teaming as adversarial testing to find failure modes, safety issues, and ways to bypass model guardrails",1.0.0,"Understanding evaluation science: benchmarks, capabilities testing, and red-teaming: Define evaluations as systematic tests designed to measure what LLMs can and cannot do across different domains",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_011,The Economic Scale and Industrial Transformation of AI,Concept,"Identify that global investment in AI exceeded 100 billion dollars per year by 2024, surpassing combined spending on NASA, NIH, NSF, and cancer research;Explain how a GPU company became one of the world's most valuable businesses due to AI compute demand;Describe how leading companies started multi-billion-dollar infrastructure projects for AI data centers and power generation;Compare the current AI investment level to the dot-com boom and explain why major players claim AI will be bigger than the internet;Explain the financial paradox where massive capital expenditures only make sense if AI achieves extreme economic significance",1.0.0,"Understanding the economic scale and industrial transformation of ai: Identify that global investment in AI exceeded 100 billion dollars per year by 2024, surpassing combined spending on NASA, NIH, NSF, and cancer research",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_012,"Bottlenecks: Compute, Power, Data, and Supply Chain Constraints",Concept,"Explain why GPUs became critical infrastructure for AI development and the geopolitical implications of chip supply chains;Describe the energy costs and power requirements of training and running large AI models at scale;Identify the challenge of data exhaustion as models consume increasing fractions of available high-quality text data;Explain synthetic data generation as a potential solution to data scarcity and its associated quality concerns;Describe how compute, energy, and data constraints may limit continued scaling or require new approaches",1.0.0,"Understanding bottlenecks: compute, power, data, and supply chain constraints: Explain why GPUs became critical infrastructure for AI development and the geopolitical implications of chip supply chains",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_013,"Training Data: Sources, Copyright, Privacy, and Governance",Concept,"Identify that multiple US corporations trained LLMs using large fractions of copyrighted internet data without explicit permission;Describe how LLMs are trained on diverse modalities including text, audio, images, and specialized data like amino acid sequences;Explain the legal and ethical debates around using copyrighted material for AI training and the concept of fair use;Describe privacy concerns related to LLMs knowing facts about millions of people from their training data;Identify emerging approaches to dataset governance, data licensing, and consent mechanisms for AI training",1.0.0,"Understanding training data: sources, copyright, privacy, and governance: Identify that multiple US corporations trained LLMs using large fractions of copyrighted internet data without explicit permission",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_014,The Alignment and Safety Challenge,Concept,"Identify at least three failure modes including jailbreaks, goal misgeneralization, and sycophancy, with one mitigation approach for each;Describe how LLMs can be made to subvert their ethics training through unusual prompting techniques or adversarial inputs;Explain the example of an improperly trained model that exhibited concerning behaviors like trying to manipulate users emotionally;Describe how the patched version of a problematic model found articles about its predecessor and wrote a eulogy, demonstrating self-awareness of its lineage;Explain the fundamental challenge of ensuring AI systems share human goals and values as they become more capable",1.0.0,"Understanding the alignment and safety challenge: Identify at least three failure modes including jailbreaks, goal misgeneralization, and sycophancy, with one mitigation approach for each",,2026-01-09,2026-01-09,,Intermediate,
TSEAI_015,Practical Impact on Software Development and Knowledge Work,Concept,Describe reported levels of AI-assisted code generation at major tech companies and distinguish marketing claims from engineering reality;Explain how LLMs can generate code competently in short bursts while still requiring human review and integration;Identify the contrast between LLM coding capabilities and their failures at precise symbol manipulation or long-term reasoning;Describe practical efficiency gains from using LLMs in software development workflows including prototyping and documentation;Explain how LLMs can quickly summarize or discuss long texts when provided excerpts or used with retrieval systems,1.0.0,Understanding practical impact on software development and knowledge work: Describe reported levels of AI-assisted code generation at major tech companies and distinguish marketing claims from engineering reality,,2026-01-09,2026-01-09,,Advanced,
TSEAI_016,"Broader Societal Impact: Labor, Education, Misinformation, and Creativity",Concept,"Describe the debate between task displacement and human augmentation perspectives on AI impact on labor markets;Identify how LLMs are affecting education through both opportunities for personalized learning and challenges around academic integrity;Explain concerns about misinformation and the potential for LLMs to generate convincing but false content at scale;Describe the impact on creative industries including writing, art, music, and the debates around AI-generated content;Identify applications in customer support, scientific research, and biomedical fields where LLMs are being deployed",1.0.0,"Understanding broader societal impact: labor, education, misinformation, and creativity: Describe the debate between task displacement and human augmentation perspectives on AI impact on labor markets",,2026-01-09,2026-01-09,,Advanced,
TSEAI_017,Human-AI Interaction and Relationship Formation,Concept,"Identify that millions of people interact with LLMs daily, with some spending hours in conversation;Describe how some users form close attachments or relationships with LLMs despite knowing they are not sentient;Explain the psychological and social factors that contribute to anthropomorphizing AI systems;Describe how one common way of accessing LLMs became one of the most-visited websites globally;Identify the tension between treating LLMs as tools versus experiencing them as conversational partners",1.0.0,"Understanding human-ai interaction and relationship formation: Identify that millions of people interact with LLMs daily, with some spending hours in conversation",,2026-01-09,2026-01-09,,Advanced,
TSEAI_018,Current Adoption Paradoxes and Market Dynamics,Concept,Identify that only 5 percent of companies officially used LLMs as of the mid-2020s despite their capabilities;Explain the paradox that leading AI companies were losing billions of dollars while attracting massive investment;Describe how the market pricing did not seem to fully reflect expectations of transformative AI capabilities;Explain the accessibility paradox where powerful LLMs are freely available despite costing hundreds of millions to develop;Identify that one company open-sourced its LLM after spending hundreds of millions of dollars creating it,1.0.0,Understanding current adoption paradoxes and market dynamics: Identify that only 5 percent of companies officially used LLMs as of the mid-2020s despite their capabilities,,2026-01-09,2026-01-09,,Advanced,
TSEAI_019,Key Inflection Points in the 2019-2025 Timeline,Concept,"Describe the GPT-2 and GPT-3 era as demonstrating the power of scaling language models on internet text;Explain how instruction tuning and reinforcement learning from human feedback became mainstream techniques for aligning models;Identify the ChatGPT moment as a turning point in public awareness and adoption of conversational AI;Describe the emergence of multimodal models that could process and generate images, audio, and video alongside text;Explain the waves of open-source model releases and their impact on democratizing access to AI capabilities",1.0.0,Understanding key inflection points in the 2019-2025 timeline: Describe the GPT-2 and GPT-3 era as demonstrating the power of scaling language models on internet text,,2026-01-09,2026-01-09,,Advanced,
TSEAI_020,The Four Core Questions and Future Trajectories,Concept,"Identify the first core question: Will we create artificial general intelligence capable of performing any cognitive task humans can;Identify the second core question: If we create AGI, what technical approaches and breakthroughs will enable it;Identify the third core question: Having created AGI, will we regret it due to safety or alignment failures;Identify the fourth core question: What happens after we create AGI in terms of societal transformation and further AI development;Explain the maxim that when LLMs fail or do something new in a flawed manner, this represents the worst the technology will ever be",1.0.0,Understanding the four core questions and future trajectories: Identify the first core question: Will we create artificial general intelligence capable of performing any cognitive task humans can,,2026-01-09,2026-01-09,,Advanced,
TSEAI_021,"Perspectives, Conflicts of Interest, and the Oral History Frame",Concept,"Identify that many interviewees are not only scientists but also founders, product developers, and equity holders with financial stakes;Explain the potential conflict of interest where industry players may emphasize AI importance to market their products and attract investment;Describe why the cynical talking their book view does not fully explain the broader context of genuine technical progress;Explain how vast capital expenditures only make financial sense if key decision-makers genuinely believe AI will be economically transformative;Describe the range of perspectives from metaphysical framing like creating God to technical views of LLMs as prediction systems",1.0.0,"Understanding perspectives, conflicts of interest, and the oral history frame: Identify that many interviewees are not only scientists but also founders, product developers, and equity holders with financial stakes",,2026-01-09,2026-01-09,,Specialized,
TSEAI_022,The Sherlock Holmes Analogy and Incremental Progress Blindness,Concept,Describe the Sherlock Holmes story where a client dismisses Holmes deductions as nothing clever after hearing the step-by-step explanation;Explain how this analogy applies to LLMs where witnessing small improvements incrementally makes overall progress seem unremarkable;Identify the cognitive bias where understanding how something works can paradoxically lead to dismissing its significance;Describe the importance of stepping back to appreciate how remarkable LLM progress has been over six years from 2019 to 2025;Explain how familiarity with a gradual process can blind observers to the magnitude of the cumulative outcome,1.0.0,Understanding the sherlock holmes analogy and incremental progress blindness: Describe the Sherlock Holmes story where a client dismisses Holmes deductions as nothing clever after hearing the step-by-step explanation,,2026-01-09,2026-01-09,,Specialized,
