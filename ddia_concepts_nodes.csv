:ID,name,:LABEL,understanding_criteria,schema_version,description,author,creation_date,last_updated,total_concepts:int,level,prerequisites
ddia_v1,Designing Data-Intensive Applications,Curriculum,,1.0.0,Navigate the fundamental principles of storing and processing data,,2026-01-06,2026-01-06,30,,
DDIA_001,Reliability in Data Systems,Concept,"Define reliability as the system continuing to work correctly even when faults occur, including hardware faults, software errors, and human errors;Distinguish between faults (component deviations from spec) and failures (system stops providing required service to users);Identify three main categories of faults: hardware faults, software errors, and human errors with examples of each;Explain how hardware faults can be addressed through redundancy and how cloud platforms use software fault-tolerance techniques;Describe systematic software errors as bugs that are harder to anticipate than random hardware faults and require different mitigation strategies;List approaches for minimizing human errors including good design, decoupling, thorough testing, quick recovery, detailed monitoring, and training;Given a fault scenario, propose appropriate mitigation strategies and explain how you would detect the fault in production;Explain fault injection and chaos testing as techniques for validating system reliability under adverse conditions;Describe graceful degradation as maintaining partial functionality when components fail rather than complete system failure",1.0.0,"Understanding reliability in data systems: Define reliability as the system continuing to work correctly even when faults occur, including hardware faults, software errors, and human errors",,2026-01-06,2026-01-06,,Foundational,
DDIA_002,Scalability and Performance,Concept,"Define scalability as a system ability to cope with increased load while maintaining acceptable performance;Identify load parameters as numbers describing system load such as requests per second, read-to-write ratio, cache hit rate, or concurrent users;Given a workload description, select 2-3 relevant load parameters and justify why they matter for that specific system;Distinguish between response time (what client sees) and latency (duration request waits to be handled);Explain why percentiles (p50, p95, p99) are better than averages for measuring response time and setting SLOs;Describe how queueing effects and head-of-line blocking contribute to tail latency in distributed systems;Compare scaling up (vertical scaling) and scaling out (horizontal scaling) with trade-offs for each approach;Explain elastic systems as automatically adding resources when load increases versus manual scaling with more predictable behavior;Given percentile targets for response time, propose architectural changes such as caching, batching, async processing, or partitioning and predict their impact;Describe capacity planning as estimating future resource needs based on growth projections and load parameters",1.0.0,Understanding scalability and performance: Define scalability as a system ability to cope with increased load while maintaining acceptable performance,,2026-01-06,2026-01-06,,Foundational,
DDIA_003,Maintainability and Operational Excellence,Concept,"Define maintainability as designing systems to minimize pain during maintenance by different people over time;Identify three key design principles for maintainability: operability, simplicity, and evolvability;Explain operability as making life easy for operations teams through good monitoring, automation, documentation, runbooks, and predictable behavior;Describe observability as the ability to understand system internal state from external outputs using logs, metrics, and traces;Define simplicity as managing complexity by removing accidental complexity while keeping essential complexity;Distinguish between accidental complexity (arising from implementation choices) and essential complexity (inherent in the problem domain);Explain that abstraction is a key tool for removing accidental complexity by hiding implementation details behind clean interfaces;Define evolvability as making it easy to adapt the system to changing requirements through modular design and loose coupling;Describe operational practices including incident response, on-call rotations, deployment strategies, and feature flags;Given a complex system design, identify sources of accidental complexity and propose simplifications through better abstractions",1.0.0,Understanding maintainability and operational excellence: Define maintainability as designing systems to minimize pain during maintenance by different people over time,,2026-01-06,2026-01-06,,Foundational,
DDIA_004,Data Model Selection and Trade-offs,Concept,"Describe the relational model as organizing data into relations (tables) where each relation is a collection of tuples (rows);Explain the object-relational mismatch as the impedance mismatch between object-oriented code and relational tables requiring translation layers;Identify that document databases like MongoDB and CouchDB store data in self-contained documents using JSON, XML, or binary variants;Explain that document models can reduce impedance mismatch by storing nested records within parent record without joins;Describe many-to-one relationships as multiple records referring to a single record, typically better handled by relational databases with joins;Explain that document databases make joins difficult or impossible, pushing join logic to application code;Compare schema-on-write (relational) where schema is explicit and enforced on write versus schema-on-read (document) where schema is implicit and interpreted on read;Given an example domain such as e-commerce or social network, choose between relational, document, or graph model and justify based on access patterns and data relationships;Identify that document databases work well for data with document-like structure, one-to-many relationships, and few relationships between documents;Recognize that relational databases work better for many-to-many relationships, complex interconnected data, and strong consistency requirements",1.0.0,Understanding data model selection and trade-offs: Describe the relational model as organizing data into relations (tables) where each relation is a collection of tuples (rows),,2026-01-06,2026-01-06,,Foundational,
DDIA_005,Query Language Paradigms,Concept,"Define declarative query languages as specifying the pattern of desired results without specifying how to achieve the goal;Define imperative query languages as specifying operations in a particular order to achieve results;Identify SQL as a declarative query language where you specify conditions for results and the database engine determines execution plan;Explain that declarative languages hide implementation details, allowing database engine to optimize without changing queries;Describe how declarative languages are more amenable to parallel execution than imperative code because they do not specify order;Recognize CSS and XSL as declarative languages for styling web pages that specify what the result should look like;Explain MapReduce as a programming model for batch processing across many machines using map and reduce functions;Identify that MapReduce is neither fully declarative nor fully imperative but somewhere in between with restricted operations",1.0.0,Understanding query language paradigms: Define declarative query languages as specifying the pattern of desired results without specifying how to achieve the goal,,2026-01-06,2026-01-06,,Foundational,
DDIA_006,Graph Data Models,Concept,"Identify graph models as appropriate when many-to-many relationships are very common and connections between data are as important as the data itself;Describe a property graph model as consisting of vertices (nodes) and edges (relationships) where both can have properties;Explain that in property graphs, any vertex can have an edge connecting it to any other vertex with no schema restrictions;Describe Cypher as a declarative query language for property graphs, created for Neo4j database;Explain triple-stores as storing data in three-part statements: subject, predicate, object forming a graph structure;Identify SPARQL as a query language for triple-stores using RDF data model for semantic web applications;Describe Datalog as a foundation for query languages using predicates and rules for recursive queries;Recognize that graph databases are useful for evolvability when application requirements change and new relationships need to be added",1.0.0,Understanding graph data models: Identify graph models as appropriate when many-to-many relationships are very common and connections between data are as important as the data itself,,2026-01-06,2026-01-06,,Foundational,
DDIA_007,Log-Structured Storage Engines,Concept,"Define a log as an append-only sequence of records that forms the basis for many storage systems;Explain hash indexes as in-memory hash maps where keys map to byte offsets in data file for fast lookups;Describe compaction as throwing away duplicate keys in log segments and keeping only the most recent update for each key;Identify SSTables (Sorted String Tables) as log segments where key-value pairs are sorted by key enabling efficient operations;Explain that SSTables enable efficient merging using mergesort algorithm to combine multiple sorted segments;Describe LSM-Trees (Log-Structured Merge-Trees) as storage engines using SSTables and memtables for write-optimized storage;Explain that writes go to an in-memory memtable which is written to disk as SSTable when it gets too big;Identify that LSM-Trees optimize for write throughput by turning random writes into sequential writes on disk;Describe bloom filters as space-efficient data structures for testing whether an element is in a set, used to avoid unnecessary disk reads;Explain read amplification, write amplification, and space amplification as trade-offs in storage engine design",1.0.0,Understanding log-structured storage engines: Define a log as an append-only sequence of records that forms the basis for many storage systems,,2026-01-06,2026-01-06,,Foundational,
DDIA_008,B-Tree Storage Engines,Concept,"Describe B-Trees as breaking database into fixed-size blocks or pages, traditionally 4KB in size, organized in a tree structure;Explain that B-Trees organize pages in a tree structure where each page contains keys and references to child pages;Identify that B-Trees have a branching factor, typically several hundred, determining number of references per page;Explain that B-Trees update pages in place, overwriting pages on disk rather than appending like log-structured storage;Describe write-ahead log (WAL) as an append-only file for crash recovery in B-Trees, ensuring durability before page updates;Explain fsync semantics and durability guarantees including the trade-off between performance and data safety;Identify that B-Trees are optimized for read performance and are the most widely used indexing structure in relational databases;Compare LSM-Trees as typically faster for writes while B-Trees are faster for reads due to not needing to check multiple segments;Explain that LSM-Trees can be compressed better and often have lower write amplification than B-Trees;Describe page-level compression techniques in B-Trees including prefix compression and dictionary encoding",1.0.0,"Understanding b-tree storage engines: Describe B-Trees as breaking database into fixed-size blocks or pages, traditionally 4KB in size, organized in a tree structure",,2026-01-06,2026-01-06,,Foundational,
DDIA_009,Secondary Indexes and Multi-Column Access,Concept,"Define secondary indexes as additional data structures that allow efficient lookups by columns other than the primary key;Explain that secondary indexes do not uniquely identify a record and may point to multiple matching records;Describe B-Tree secondary indexes as maintaining a separate B-Tree for each indexed column with pointers to primary data;Explain inverted indexes as mapping from terms to documents containing those terms, commonly used for full-text search;Describe multi-column indexes including concatenated indexes and multi-dimensional indexes for geospatial data;Identify trade-offs of secondary indexes including storage overhead and write performance impact;Explain covering indexes as including additional columns in the index to avoid accessing the main table;Describe how secondary indexes interact with different storage engines including B-Trees and LSM-Trees",1.0.0,Understanding secondary indexes and multi-column access: Define secondary indexes as additional data structures that allow efficient lookups by columns other than the primary key,,2026-01-06,2026-01-06,,Intermediate,
DDIA_010,OLTP versus OLAP Workloads,Concept,"Define OLTP (Online Transaction Processing) as interactive queries with low latency, small number of records per query, and frequent writes;Define OLAP (Online Analytic Processing) as queries scanning large number of records, aggregating data for analytics, and infrequent writes;Explain data warehousing as a separate database optimized for analytics with data extracted from OLTP systems via ETL processes;Describe star schema as a data warehouse schema with fact table in center and dimension tables around it;Explain snowflake schema as a variation where dimensions are further broken down into subdimensions for normalization;Identify that OLTP systems are optimized for transaction processing while OLAP systems are optimized for analytical queries;Recognize that data warehouses allow analytics without impacting OLTP system performance by separating workloads;Describe ETL (Extract, Transform, Load) as the process of copying data from OLTP systems to data warehouse",1.0.0,"Understanding oltp versus olap workloads: Define OLTP (Online Transaction Processing) as interactive queries with low latency, small number of records per query, and frequent writes",,2026-01-06,2026-01-06,,Intermediate,
DDIA_011,Column-Oriented Storage,Concept,"Explain column-oriented storage as storing all values from each column together rather than all values from each row;Describe how column storage reduces data that needs to be read from disk for analytics queries that only access few columns;Explain column compression techniques like bitmap encoding and run-length encoding that work well with sorted column data;Identify that column storage is optimized for read-heavy analytics workloads with high compression ratios;Describe how writes are more challenging in column storage, often using write-optimized buffers or delta stores before merging;Explain materialized views as precomputed query results stored on disk to speed up common analytical queries;Describe data cubes as a special case of materialized view, a grid of aggregates grouped by different dimensions;Identify trade-offs of materialized views including storage cost and staleness of precomputed results",1.0.0,Understanding column-oriented storage: Explain column-oriented storage as storing all values from each column together rather than all values from each row,,2026-01-06,2026-01-06,,Intermediate,
DDIA_012,Caching Strategies,Concept,"Define caching as storing frequently accessed data in faster storage layer to reduce latency and load on slower systems;Identify different cache locations including client-side caches, CDN caches, application caches, and database caches;Explain cache invalidation strategies including time-to-live (TTL), write-through, write-back, and explicit invalidation;Describe the cache invalidation problem as one of the hardest problems in computer science due to consistency challenges;Explain cache hit rate as the percentage of requests served from cache and its impact on overall system performance;Identify cache eviction policies including LRU (Least Recently Used), LFU (Least Frequently Used), and FIFO;Describe cache warming as preloading cache with data before it is needed to avoid cold start problems;Explain cache stampede as the problem when many requests for expired cache entry hit backend simultaneously",1.0.0,Understanding caching strategies: Define caching as storing frequently accessed data in faster storage layer to reduce latency and load on slower systems,,2026-01-06,2026-01-06,,Intermediate,
DDIA_013,Encoding and Schema Evolution,Concept,"Define encoding (serialization) as translating in-memory data structures to byte sequences for storage or transmission;Identify JSON, XML, and CSV as textual formats that are human-readable but have ambiguities, no schema enforcement, and larger size;Describe binary encodings like Thrift, Protocol Buffers, and Avro as more compact than textual formats with schema support;Explain that schemas are valuable for documentation, code generation for statically typed languages, and enabling evolution;Describe forward compatibility as old code can read data written by new code by ignoring unknown fields;Describe backward compatibility as new code can read data written by old code by providing default values for new fields;Explain that Avro uses writer schema and reader schema allowing schema evolution without field tags or IDs;Identify that schema evolution allows gradual upgrades without downtime in rolling deployments;Describe schema registry as a centralized service for managing and versioning schemas in distributed systems",1.0.0,Understanding encoding and schema evolution: Define encoding (serialization) as translating in-memory data structures to byte sequences for storage or transmission,,2026-01-06,2026-01-06,,Intermediate,
DDIA_014,Modes of Dataflow,Concept,"Identify three main modes of dataflow: through databases, through services (REST/RPC), and through message passing;Explain dataflow through databases as one process writing encoded data and another process reading it later with schema evolution considerations;Describe REST as a design philosophy for web services using HTTP features, emphasizing simple data formats, URLs as resources, and stateless requests;Explain RPC (Remote Procedure Call) as trying to make remote network calls look like local function calls;Identify problems with RPC including network unpredictability, timeouts, idempotency requirements, and different semantics than local calls;Describe message-passing dataflow as asynchronous communication through message brokers or queues decoupling sender and recipient;Explain that message brokers act as buffers improving reliability and allowing sender and recipient to be decoupled in time;Identify that each dataflow mode has different characteristics for compatibility, evolution, and failure handling",1.0.0,"Understanding modes of dataflow: Identify three main modes of dataflow: through databases, through services (REST/RPC), and through message passing",,2026-01-06,2026-01-06,,Intermediate,
DDIA_015,Replication: Leaders and Followers,Concept,"Define replication as keeping a copy of the same data on multiple machines connected via network for fault tolerance and scalability;Explain leader-based replication where one replica is designated leader accepting writes and others are followers replicating changes;Describe synchronous replication as waiting for follower confirmation before reporting write success to user;Describe asynchronous replication as not waiting for follower confirmation allowing writes to proceed faster with eventual consistency;Explain the trade-off: synchronous replication guarantees up-to-date copy but slows down writes if follower unavailable;Describe setting up new followers by taking snapshot of leader, copying to follower, then follower requests changes since snapshot position;Explain handling follower failure through catch-up recovery using replication log to replay missed changes;Describe handling leader failure through failover: promoting a follower to be new leader and reconfiguring clients;Identify challenges in failover including split brain, data loss from asynchronous replication, and determining appropriate timeout",1.0.0,Understanding replication: leaders and followers: Define replication as keeping a copy of the same data on multiple machines connected via network for fault tolerance and scalability,,2026-01-06,2026-01-06,,Intermediate,
DDIA_016,Replication Lag and Consistency Guarantees,Concept,"Define replication lag as the delay between a write on leader and reflection on followers in asynchronous replication;Explain read-after-write consistency as users seeing their own writes immediately after submission;Describe techniques for read-after-write consistency like reading user own profile from leader or tracking timestamp of last update;Define monotonic reads as guaranteeing users will not see data moving backward in time across multiple reads;Explain that monotonic reads can be achieved by ensuring each user always reads from same replica;Define consistent prefix reads as guaranteeing that if sequence of writes happens in certain order, anyone reading will see them in same order;Identify that these problems arise from eventual consistency in asynchronous replication;Recognize that stronger guarantees like transactions can solve these problems but with performance cost;Given a failure scenario with replication lag, predict what anomalies users might observe and propose mitigation strategies",1.0.0,Understanding replication lag and consistency guarantees: Define replication lag as the delay between a write on leader and reflection on followers in asynchronous replication,,2026-01-06,2026-01-06,,Intermediate,
DDIA_017,Multi-Leader and Leaderless Replication,Concept,"Describe multi-leader replication as allowing more than one node to accept writes concurrently;Identify use cases for multi-leader replication including multi-datacenter operation, offline clients, and collaborative editing;Explain that multi-leader replication introduces write conflicts when same data is modified in two different locations concurrently;Describe conflict resolution strategies including last write wins, merging values, custom application logic, and CRDTs;Explain leaderless replication as allowing any replica to accept writes from clients without leader coordination;Describe quorum reads and writes where reads and writes are sent to multiple nodes in parallel;Define quorum condition as w plus r greater than n where w is write nodes, r is read nodes, n is total replicas;Explain sloppy quorum as accepting writes even when quorum nodes unavailable with hinted handoff later;Describe version vectors and conflict detection mechanisms in leaderless replication",1.0.0,Understanding multi-leader and leaderless replication: Describe multi-leader replication as allowing more than one node to accept writes concurrently,,2026-01-06,2026-01-06,,Intermediate,
DDIA_018,Partitioning Strategies,Concept,"Define partitioning (sharding) as breaking data into partitions to spread data and query load across multiple machines;Explain partitioning by key range as assigning continuous range of keys to each partition enabling efficient range queries;Identify that key range partitioning risks hot spots if access pattern not uniform or keys not well distributed;Describe partitioning by hash of key as using hash function to determine partition for a key distributing load evenly;Explain that hash partitioning loses ability to do efficient range queries because adjacent keys are scattered across partitions;Describe skewed workload as some partitions having more load than others due to uneven data or access distribution;Identify hot spot as a partition with disproportionately high load that can become a bottleneck;Explain consistent hashing and virtual nodes as techniques for more flexible partition assignment and rebalancing;Given a workload description, choose between key range and hash partitioning and justify the choice",1.0.0,Understanding partitioning strategies: Define partitioning (sharding) as breaking data into partitions to spread data and query load across multiple machines,,2026-01-06,2026-01-06,,Intermediate,
DDIA_019,Partitioning and Secondary Indexes,Concept,Explain that secondary indexes do not map neatly to partitions making them more complicated in distributed systems;Describe document-partitioned indexes (local indexes) where each partition maintains its own secondary indexes covering only its data;Explain that document-partitioned indexes require scatter-gather queries across all partitions to find all matching documents;Describe term-partitioned indexes (global indexes) where secondary index is partitioned separately from primary data;Identify that term-partitioned indexes make reads more efficient by querying only relevant index partitions but writes slower and more complicated;Explain that global indexes are usually updated asynchronously to avoid coordination overhead making them eventually consistent;Compare trade-offs between local and global secondary indexes for different query patterns,1.0.0,Understanding partitioning and secondary indexes: Explain that secondary indexes do not map neatly to partitions making them more complicated in distributed systems,,2026-01-06,2026-01-06,,Advanced,
DDIA_020,Rebalancing Partitions,Concept,"Define rebalancing as moving data from one node to another when nodes are added, removed, or load becomes uneven;Identify requirements for rebalancing: fair data distribution, continued read and write acceptance, and minimal data movement;Explain why hash mod n is a bad strategy because most keys need to move when n changes causing massive data movement;Describe fixed number of partitions as creating many more partitions than nodes then assigning partitions to nodes;Explain dynamic partitioning as splitting partitions when they grow too large and merging when they shrink below threshold;Describe proportional partitioning as making number of partitions proportional to number of nodes;Identify that automatic rebalancing is convenient but can overload network and nodes if not careful during high load;Explain that manual rebalancing gives administrators control but requires more operational work and expertise",1.0.0,"Understanding rebalancing partitions: Define rebalancing as moving data from one node to another when nodes are added, removed, or load becomes uneven",,2026-01-06,2026-01-06,,Advanced,
DDIA_021,ACID Transaction Properties,Concept,"Define transaction as a way for application to group several reads and writes into logical unit executed as one operation;Explain Atomicity as all-or-nothing property: either all writes in transaction succeed or all are rolled back on error;Describe Consistency as application-specific notion that certain invariants must always be true before and after transaction;Define Isolation as concurrently executing transactions are isolated from each other and do not interfere;Explain Durability as promise that once transaction commits successfully, data is not lost even if hardware faults or database crashes;Identify that ACID properties provide safety guarantees but with performance costs compared to weaker guarantees;Recognize that different databases implement ACID properties differently with varying levels of guarantees;Explain write-ahead logging and fsync as mechanisms for ensuring durability in single-node databases",1.0.0,Understanding acid transaction properties: Define transaction as a way for application to group several reads and writes into logical unit executed as one operation,,2026-01-06,2026-01-06,,Advanced,
DDIA_022,Weak Isolation Levels,Concept,"Define read committed isolation as guaranteeing no dirty reads and no dirty writes;Explain dirty read as reading data written by transaction that has not yet committed potentially seeing data that gets rolled back;Explain dirty write as overwriting data written by transaction that has not yet committed causing lost updates;Describe snapshot isolation as each transaction reads from consistent snapshot of database at start of transaction;Explain that snapshot isolation is implemented using multi-version concurrency control (MVCC) keeping multiple versions of objects;Identify that snapshot isolation prevents read skew where transaction sees inconsistent data due to concurrent updates;Define lost update problem as two transactions read-modify-write and one overwrites the other without incorporating changes;Describe solutions for lost updates including atomic operations, explicit locking, automatic detection, and compare-and-set;Given an application requirement, identify which isolation level is sufficient and explain why",1.0.0,Understanding weak isolation levels: Define read committed isolation as guaranteeing no dirty reads and no dirty writes,,2026-01-06,2026-01-06,,Advanced,
DDIA_023,Write Skew and Phantoms,Concept,Define write skew as two transactions reading same objects then updating different objects violating an invariant;Explain that write skew is a generalization of lost update problem affecting multiple objects;Identify that write skew cannot be prevented by atomic operations or automatic lost update detection;Describe phantom as a write in one transaction changes result of search query in another transaction;Explain that snapshot isolation does not prevent write skew or phantoms requiring stronger isolation;Identify that serializable isolation is needed to prevent write skew and phantoms;Describe materializing conflicts as turning phantom into lock conflict on concrete set of rows;Explain predicate locks as locks that apply to all objects matching a search condition,1.0.0,Understanding write skew and phantoms: Define write skew as two transactions reading same objects then updating different objects violating an invariant,,2026-01-06,2026-01-06,,Advanced,
DDIA_024,Serializability Implementation Approaches,Concept,"Define serializability as strongest isolation level guaranteeing transactions execute as if serial with no concurrency;Describe actual serial execution as literally executing transactions one at a time on single thread;Explain that actual serial execution requires transactions to be small and fast and dataset to fit in memory;Describe two-phase locking (2PL) as transactions acquiring locks before reading or writing objects;Explain that 2PL has shared locks for reading and exclusive locks for writing with strict lock ordering;Identify that 2PL has significant performance penalty due to lock overhead, reduced concurrency, and potential deadlocks;Describe deadlock detection and prevention strategies including timeout-based and wait-for graph approaches;Describe serializable snapshot isolation (SSI) as optimistic concurrency control detecting conflicts at commit time;Explain that SSI has better performance than 2PL because transactions can proceed without blocking;Compare the three approaches based on what anomalies are prevented, how conflicts are detected, and performance characteristics",1.0.0,Understanding serializability implementation approaches: Define serializability as strongest isolation level guaranteeing transactions execute as if serial with no concurrency,,2026-01-06,2026-01-06,,Advanced,
DDIA_025,Distributed Transactions and Atomic Commit,Concept,"Explain the challenge of atomic commit across multiple nodes where some may succeed and others fail;Describe two-phase commit (2PC) as algorithm for atomic commit across multiple nodes using coordinator;Explain that 2PC uses prepare phase where participants vote and commit phase where coordinator decides outcome;Identify that 2PC is a blocking protocol: if coordinator crashes after prepare, participants must wait unable to commit or abort;Describe coordinator failure recovery using transaction log to determine outcome of in-doubt transactions;Explain why distributed transactions are often avoided in practice due to performance overhead and operational complexity;Describe saga pattern as alternative to distributed transactions using compensating transactions for rollback;Explain transactional outbox pattern as ensuring atomic write to database and message publication",1.0.0,Understanding distributed transactions and atomic commit: Explain the challenge of atomic commit across multiple nodes where some may succeed and others fail,,2026-01-06,2026-01-06,,Advanced,
DDIA_026,Unreliable Networks in Distributed Systems,Concept,"Explain that distributed systems communicate over asynchronous packet networks where packets can be lost, delayed, or duplicated;Identify that network problems are common in practice not just theoretical concerns requiring defensive design;Describe that when sending request and not getting response, it is impossible to know whether request was lost, remote node is down, or response was lost;Explain timeout as the only reliable way to detect faults in asynchronous networks;Describe the trade-off in choosing timeout duration: long timeout means long wait before declaring node dead, short timeout risks false positives;Explain that network congestion and queueing at switches and network interfaces cause variable delays in packet delivery;Identify that synchronous networks like telephone provide bounded delays but are inefficient for bursty data traffic;Describe retry strategies including exponential backoff and jitter to avoid thundering herd problems",1.0.0,"Understanding unreliable networks in distributed systems: Explain that distributed systems communicate over asynchronous packet networks where packets can be lost, delayed, or duplicated",,2026-01-06,2026-01-06,,Advanced,
DDIA_027,Unreliable Clocks in Distributed Systems,Concept,"Distinguish between time-of-day clocks (wall-clock time) and monotonic clocks (elapsed time);Explain that time-of-day clocks can jump backward or forward due to NTP synchronization making them unsuitable for ordering;Describe monotonic clocks as always moving forward suitable for measuring durations and timeouts;Identify that clock synchronization accuracy is limited typically tens of milliseconds at best in datacenter;Explain that relying on synchronized clocks is dangerous because clocks can be wrong and drift over time;Describe problems with using timestamps for ordering events in distributed systems including clock skew;Explain that logical clocks based on incrementing counters are safer than physical clocks for ordering events;Identify process pauses due to garbage collection, virtual machine suspension, or OS scheduling as another timing issue",1.0.0,Understanding unreliable clocks in distributed systems: Distinguish between time-of-day clocks (wall-clock time) and monotonic clocks (elapsed time),,2026-01-06,2026-01-06,,Advanced,
DDIA_028,Distributed System Truth and Consensus,Concept,"Explain that in distributed systems truth is defined by the majority through quorum decisions not individual nodes;Describe the problem of split brain where two nodes both believe they are the leader causing conflicts;Explain that fencing tokens can prevent split brain by ensuring old leader cannot interfere after new leader elected;Define Byzantine faults as nodes that may lie, send corrupted responses, or behave maliciously;Identify that most systems assume nodes are honest but may be unreliable using crash-recovery model;Describe system model as an abstraction defining what things can go wrong including timing and fault assumptions;Explain that algorithms can be proven correct within a system model but real systems may violate assumptions;Recognize that empirical testing and chaos engineering are needed to validate system behavior under real conditions",1.0.0,Understanding distributed system truth and consensus: Explain that in distributed systems truth is defined by the majority through quorum decisions not individual nodes,,2026-01-06,2026-01-06,,Specialized,
DDIA_029,Linearizability,Concept,"Define linearizability as making a system appear as if there is only one copy of data with atomic operations;Explain that linearizability provides recency guarantee: once new value written or read, all subsequent reads see that value or newer;Describe linearizability as a consistency model that makes replicated data appear as single copy;Identify use cases requiring linearizability: locking and leader election, constraints and uniqueness guarantees;Explain that single-leader replication is potentially linearizable if reads from leader or synchronously updated followers;Describe that consensus algorithms like Zookeeper and etcd provide linearizability through coordination;Explain the CAP theorem trade-off: in network partition must choose between linearizability and availability;Identify that many distributed databases sacrifice linearizability for better performance and availability",1.0.0,Understanding linearizability: Define linearizability as making a system appear as if there is only one copy of data with atomic operations,,2026-01-06,2026-01-06,,Specialized,
DDIA_030,Causality and Ordering,Concept,Define causality as relationship between events where one event happens before another and influences it;Explain that causality imposes partial ordering on events: cause comes before effect but concurrent events are not ordered;Describe causal consistency as weaker than linearizability but still providing meaningful ordering of related events;Identify that causal consistency does not have performance penalty of linearizability and works across partitions;Explain sequence numbers or timestamps can be used to provide total ordering consistent with causality;Describe Lamport timestamps as a simple method for generating sequence numbers consistent with causality;Explain that Lamport timestamps provide total ordering but do not solve distributed consensus problems;Identify total order broadcast as a protocol for delivering messages in same order to all nodes,1.0.0,Understanding causality and ordering: Define causality as relationship between events where one event happens before another and influences it,,2026-01-06,2026-01-06,,Specialized,
